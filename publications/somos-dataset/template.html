<!doctype html>
<html>

<head>
  <title>Innoetics</title>
  <style>
    .container {
      padding: 12px 24px;
    }

    .head-container {
      /* border: 5px solid gray; */
      border-radius: 25px;
      /* border: 1px solid; */
      border-color: rgba(0,0,0,0.2);
      text-align: center;
      padding: 16px;
      margin: 0px 200px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.3s;
    }

    .head-container:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(250, 250, 255);
    }

    .sample-container {
      margin: 18px 0;
    }

    .sample {
      /* border: 1px solid gray;
      border-radius: 10px; */
      padding: 10px;
      margin: 10px;
      width: 80%;
      position: relative;
      display: block;
      text-align: left;
    }

    .sample-title {
      font-size: 22px;
      margin: 8px 0;
      display: inline-flex;
    }

    .sample-description {
      font-size: 16px;
      margin: 8px 0;
      color: #868686e6;
    }

    .sample-audio {
      margin: 14px 0;
      display: flex;
      align-items: center;
      justify-content: left;
      padding-left: 50px;
    }

    .quotation {
      font-size: xxx-large;
      color: rgb(152, 189, 253);
      margin-top: -12px;
    }

    .transcript {
      display: inline;
      font-style: italic;
      background-color: rgb(231, 239, 253);
      padding: 15px 15px;
      border-radius: 25px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.1s;
    }

    audio {
      height: 35px;
    }

    .r-number {
      width: 24px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .r-number:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

  </style>
</head>

<body class="container">
  <script>
    function togglePlay(el) {
      if (!el.paused) {
        el.pause()
      }
      else {
        el.play();
      }
    };
  </script>
  <div class="head-container">
  <h1 style="text-align: center;">SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis</h1>
  <p style="text-align: center;font-size: 20px;">
    Georgia Maniati, 
      Alexandra Vioni,
      Nikolaos Ellinas,
      Karolos Nikitaras,
      Konstantinos Klapsas,
      June Sig Sung,
      Gunu Jho,
      Aimilios Chalamandaris and Pirros Tsiakoulis</p>
  <p style="text-align: left;font-size: 18px;"><strong>Abstract:</strong>
    We release the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be widely used to train neural MOS prediction systems which will be focused on the evaluation of modern synthesizers and will encourage further advancements in fine-grained MOS prediction. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset traditionally used in the community. Utterances are generated from 200 different TTS systems including a variety of vanilla neural acoustic models and models with prosodic variations. The vocoder remains the same, while the acoustic parameters vary. The synthesized sentences provide domain, length and phoneme coverage. Subjective evaluation scores are collected via crowdsourcing on Amazon Mechanical Turk. In this work, we present the design, value and statistical analysis of the SOMOS dataset, as well as train and evaluate 3 state-of-the-art MOS prediction models on it, namely MOSNet, LDNet and SSL.
  </p>
  </div>
