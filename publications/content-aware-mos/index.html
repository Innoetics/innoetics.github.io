<!doctype html>
<html>

<head>
  <title>Innoetics</title>
  <style>
    .container {
      padding: 12px 24px;
      overflow: scroll;
    }

    .head-container {
      /* border: 5px solid gray; */
      border-radius: 25px;
      /* border: 1px solid; */
      border-color: rgba(0,0,0,0.2);
      text-align: center;
      padding: 16px;
      margin: 0px 200px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.3s;
    }

    .head-container:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(250, 250, 255);
    }

    .tooltip {
      position: relative;
      display: inline-block;
      cursor: pointer;
    }


    .tooltip .tooltiptext {
      visibility: hidden;
      width: 120px;
      background-color: black;
      color: #fff;
      text-align: center;
      padding: 5px 0;
      border-radius: 6px;
      position: absolute;
      z-index: 1;
      cursor: pointer;
    }

    .tooltip:hover .tooltiptext {
      visibility: visible;
      cursor: pointer;
    }

    .sample-container {
      margin: 18px 0;
    }

    .sample {
      /* border: 1px solid gray;
      border-radius: 10px; */
      padding: 10px;
      margin: 10px;
      width: 80%;
      position: relative;
      display: block;
      text-align: left;
    }

    .sample-title {
      font-size: 22px;
      margin: 8px 0;
      display: inline-flex;
    }

    .sample-description {
      font-size: 16px;
      margin: 8px 0;
      color: #868686e6;
    }

    .mod-container {
      display: flex;
    }

    .sample-audio {
      margin: 10px 0;
      display: inline-block;
      align-items: center;
      justify-content: left;
      padding-left: 10px;
    }

    .quotation {
      font-size: xxx-large;
      color: rgb(152, 189, 253);
      margin-top: -12px;
    }

    .transcript {
      display: inline;
      font-style: italic;
      background-color: rgb(231, 239, 253);
      padding: 15px 15px;
      border-radius: 25px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.1s;
    }

    audio {
      height: 35px;
      display: none;
    }

    .r-number {
      width: 24px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .r-number:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .r-number-aug {
      width: 48px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .r-number-aug:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .gt {
      width: 109px;
      text-align: center;
      margin-right: 0px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .gt:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .gt.speaker {
      width: 100px;
    }

    .note {
      text-align: center;
      background-color: rgb(231, 239, 253);
      padding: 10px;
      border-radius: 25px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.1s;
    }

    .note:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .invisible {
      width: 100px;
      text-align: center;
      margin-right: 8px;
      border-radius: 25px;
      padding: 10px;
    }

    .invisible.axis {
      width: 100px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(200, 200, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    h2:hover {
      cursor: pointer;
      background: silver;
    }

    h2 {
      border: azure;
      background: bisque;
      border-radius: 10px;
      padding: 10px;
    }

    a.custom-a-href {
      color: black;
      text-decoration: none;
    }

    html {
      scroll-behavior: smooth;
    }

  </style>
</head>

<body class="container">
  <script >
    function togglePlay(el) {
      if (!el.paused) {
        el.pause()
      }
      else {
        var allAudioEls = document.getElementsByTagName("audio");
        for (let item of allAudioEls) {
            item.pause()
        }
        el.currentTime = 0;
        el.play();
      }
    };
    function showSample(event) {
      var target = event.target || event.srcElement;
      var currentDisplayAttrValue = target.parentElement.nextElementSibling.style.display;
      target.parentElement.nextElementSibling.style.display =  currentDisplayAttrValue == 'none' ? 'block' : 'none';
    };
  </script>
  <div class="head-container">
    <h1 style="text-align: center;">Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features</h1>
    <p style="text-align: center;font-size: 20px;">
      Alexandra Vioni,
        Georgia Maniati,
        Nikolaos Ellinas,
        June Sig Sung,
        Inchul Hwang,
        Aimilios Chalamandaris and 
        Pirros Tsiakoulis</p>
    <p style="text-align: left;font-size: 18px;"><strong>Abstract:</strong> Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models.
      Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input.
      In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness.
      For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome.
      We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs.
      All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations.
      Results show that the proposed additional features are beneficial in the MOS prediction task, by improving the predicted MOS scores' correlation with the ground truths, both at utterance-level and system-level predictions.</p>
    </div>
</body>
</html>
