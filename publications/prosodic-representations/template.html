<!doctype html>
<html>

<head>
  <title>Innoetics</title>
  <style>
    .container {
      padding: 12px 24px;
      overflow: scroll;
    }

    .head-container {
      /* border: 5px solid gray; */
      border-radius: 25px;
      /* border: 1px solid; */
      border-color: rgba(0,0,0,0.2);
      text-align: center;
      padding: 16px;
      margin: 0px 200px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.3s;
    }

    .head-container:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(250, 250, 255);
    }

    .tooltip {
      position: relative;
      display: inline-block;
      cursor: pointer;
    }


    .tooltip .tooltiptext {
      visibility: hidden;
      width: 120px;
      background-color: black;
      color: #fff;
      text-align: center;
      padding: 5px 0;
      border-radius: 6px;
      position: absolute;
      z-index: 1;
      cursor: pointer;
    }

    .tooltip:hover .tooltiptext {
      visibility: visible;
      cursor: pointer;
    }

    .sample-container {
      margin: 18px 0;
    }

    .sample {
      /* border: 1px solid gray;
      border-radius: 10px; */
      padding: 10px;
      margin: 10px;
      width: 80%;
      position: relative;
      display: block;
      text-align: left;
    }

    .sample-title {
      font-size: 22px;
      margin: 8px 0;
      display: inline-flex;
    }

    .sample-description {
      font-size: 16px;
      margin: 8px 0;
      color: #868686e6;
    }

    .mod-container {
      display: flex;
    }

    .sample-audio {
      margin: 10px 0;
      display: inline-block;
      align-items: center;
      justify-content: left;
      padding-left: 10px;
    }

    .quotation {
      font-size: xxx-large;
      color: rgb(152, 189, 253);
      margin-top: -12px;
    }

    .transcript {
      display: inline;
      font-style: italic;
      background-color: rgb(231, 239, 253);
      padding: 15px 15px;
      border-radius: 25px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.1s;
    }

    audio {
      height: 35px;
      display: none;
    }

    .r-number {
      width: 24px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .r-number:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .r-number-aug {
      width: 48px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .r-number-aug:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .gt {
      width: 109px;
      text-align: center;
      margin-right: 0px;
      background-color: rgb(231, 239, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    .gt:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .gt.speaker {
      width: 100px;
    }

    .gt.pred {
      width: 109px;
    }

    .note {
      text-align: center;
      background-color: rgb(231, 239, 253);
      padding: 10px;
      border-radius: 25px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
      transition: 0.1s;
    }

    .note:hover {
      box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);
      background-color: rgb(222, 230, 253);
      cursor: pointer;
    }

    .invisible {
      width: 100px;
      text-align: center;
      margin-right: 8px;
      border-radius: 25px;
      padding: 10px;
    }

    .invisible.axis {
      width: 100px;
      text-align: center;
      margin-right: 8px;
      background-color: rgb(200, 200, 253);
      border-radius: 25px;
      padding: 10px;
      box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    }

    h2:hover {
      cursor: pointer;
      background: silver;
    }

    h2 {
      border: azure;
      background: bisque;
      border-radius: 10px;
      padding: 10px;
    }

    a.custom-a-href {
      color: black;
      text-decoration: none;
    }

    html {
      scroll-behavior: smooth;
    }

  </style>
</head>

<body class="container">
  <script >
    function togglePlay(el) {
      if (!el.paused) {
        el.pause()
      }
      else {
        var allAudioEls = document.getElementsByTagName("audio");
        for (let item of allAudioEls) {
            item.pause()
        }
        el.currentTime = 0;
        el.play();
      }
    };
    function showSample(event) {
      var target = event.target || event.srcElement;
      var currentDisplayAttrValue = target.parentElement.nextElementSibling.style.display;
      target.parentElement.nextElementSibling.style.display =  currentDisplayAttrValue == 'none' ? 'block' : 'none';
    };
  </script>
  <div class="head-container">
  <h1 style="text-align: center;">Controllable speech synthesis by learning discrete phoneme-level prosodic representations</h1>
  <p style="text-align: center;font-size: 20px;">
    Nikolaos Ellinas,
    Myrsini Christidou,
    Alexandra Vioni,
    June Sig Sung,
    Aimilios Chalamandaris,
    Pirros Tsiakoulis and Paris Mastorocostas</p>
  <p style="text-align: left;font-size: 18px;"><strong>Abstract:</strong> This paper presents a method for phoneme-level prosody control of F0 and duration on a text-to-speech setup, which is based on prosodic clustering. An autoregressive attention-based model is used, incorporating a prosody encoder module which is fed discrete prosodic labels.
    Phoneme-level F0 and duration features are extracted from the speech data and discretized using unsupervised clustering in order to produce a sequence of prosodic labels.
    Prosodic control range and coverage is shown to be improved by utilizing augmentation, F0 normalization, balanced clustering for duration and speaker-independent clustering in a multispeaker setup.
    The final model enables fine-grained phoneme-level prosody control for all speakers contained in the training set, while maintaining the speaker identity.
    A prior prosody encoder is also trained, which learns the style of each speaker and enables speech synthesis without the requirement of reference audio.
    The model is also fine-tuned to unseen speakers with limited amounts of data and it is shown to maintain its prosody control capabilities, verifying that the speaker-independent prosodic clustering is effective.
    Experimental results verify that the model maintains high output speech quality and that the proposed method allows efficient prosody control within each speaker's range despite the variability that a multispeaker setting introduces.</p>
  </div>
